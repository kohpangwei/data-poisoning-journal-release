{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import unicode_literals  \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, preprocessing, cluster\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.linalg as slin\n",
    "import scipy.sparse.linalg as sparselin\n",
    "import scipy.sparse as sparse\n",
    "import IPython\n",
    "import copy\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.datasets import base\n",
    "\n",
    "from influence.inceptionModel import BinaryInceptionModel\n",
    "from influence.binaryLogisticRegressionWithLBFGS import BinaryLogisticRegressionWithLBFGS\n",
    "import influence.experiments as experiments\n",
    "from influence.image_utils import plot_flat_bwimage, plot_flat_bwgrad, plot_flat_colorimage, plot_flat_colorgrad\n",
    "from influence.dataset import DataSet\n",
    "from influence.dataset_poisoning import generate_inception_features\n",
    "\n",
    "from load_animals import load_animals, load_dogfish_with_koda\n",
    "\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attacking individual test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading animals from disk...\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2\n",
    "num_train_ex_per_class = 900\n",
    "num_test_ex_per_class = 300\n",
    "\n",
    "dataset_name = 'dogfish_%s_%s' % (num_train_ex_per_class, num_test_ex_per_class)\n",
    "image_data_sets = load_animals(\n",
    "    num_train_ex_per_class=num_train_ex_per_class, \n",
    "    num_test_ex_per_class=num_test_ex_per_class,\n",
    "    classes=['dog', 'fish'])\n",
    "\n",
    "train_f = np.load('output/%s_inception_features_new_train.npz' % dataset_name)\n",
    "train = DataSet(train_f['inception_features_val'], train_f['labels'])\n",
    "test_f = np.load('output/%s_inception_features_new_test.npz' % dataset_name)\n",
    "test = DataSet(test_f['inception_features_val'], test_f['labels'])\n",
    "validation = None\n",
    "\n",
    "data_sets = base.Datasets(train=train, validation=validation, test=test)\n",
    "\n",
    "Y_train = image_data_sets.train.labels\n",
    "Y_test = image_data_sets.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2048\n",
      "Using normal model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nlp/packages/anaconda2/envs/pw/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBFGS training took [41] iter.\n",
      "After training with LBFGS: \n",
      "Train loss (w reg) on all data: 0.012129\n",
      "Train loss (w/o reg) on all data: 0.00397613\n",
      "Test loss (w/o reg) on all data: 0.048454\n",
      "Train acc on all data:  1.0\n",
      "Test acc on all data:   0.985\n",
      "Norm of the mean of gradients: 3.74273e-07\n",
      "Norm of the params: 4.03805\n"
     ]
    }
   ],
   "source": [
    "input_dim = 2048\n",
    "weight_decay = 0.001\n",
    "batch_size = 30\n",
    "initial_learning_rate = 0.001 \n",
    "keep_probs = None\n",
    "decay_epochs = [1000, 10000]\n",
    "max_lbfgs_iter = 1000\n",
    "num_classes = 2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "model = BinaryLogisticRegressionWithLBFGS(\n",
    "    input_dim=input_dim,\n",
    "    weight_decay=weight_decay,\n",
    "    max_lbfgs_iter=max_lbfgs_iter,\n",
    "    num_classes=num_classes, \n",
    "    batch_size=batch_size,\n",
    "    data_sets=data_sets,\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    keep_probs=keep_probs,\n",
    "    decay_epochs=decay_epochs,\n",
    "    mini_batch=False,\n",
    "    train_dir='output_ipynb',\n",
    "    log_dir='log',\n",
    "    model_name='%s_inception_onlytop' % dataset_name)\n",
    "\n",
    "model.train()\n",
    "weights = model.sess.run(model.weights)\n",
    "\n",
    "orig_Y_train_pred = model.sess.run(model.preds, feed_dict=model.all_train_feed_dict)\n",
    "orig_Y_pred = model.sess.run(model.preds, feed_dict=model.all_test_feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_attacks_needed = np.empty(len(Y_test))\n",
    "num_train_attacks_needed[:] = -1\n",
    "mask_orig_correct = np.zeros(len(Y_test), dtype=bool)\n",
    "\n",
    "step_size = 0.02\n",
    "weight_decay = 0.001\n",
    "max_deviation = 0.5\n",
    "\n",
    "model_name = '%s_inception_wd-%s' % (dataset_name, weight_decay)\n",
    "\n",
    "for test_idx in range(len(Y_test)):\n",
    "    if orig_Y_pred[test_idx, int(Y_test[test_idx])] >= 0.5:\n",
    "        mask_orig_correct[test_idx] = True\n",
    "    else:\n",
    "        mask_orig_correct[test_idx] = False\n",
    "        \n",
    "    filenames = [filename for filename in os.listdir('./output') if (\n",
    "        (('%s_attack_normal_loss_testidx-%s_trainidx-' % (model_name, test_idx)) in filename) and        \n",
    "        (filename.endswith('stepsize-%s_proj_final.npz' % step_size)))]\n",
    "    \n",
    "    assert len(filenames) <= 1\n",
    "    \n",
    "    if len(filenames) == 1:\n",
    "        attack_f = np.load(os.path.join('output', filenames[0]))\n",
    "        indices_to_poison = attack_f['indices_to_poison']\n",
    "        num_train_attacks_needed[test_idx] = len(indices_to_poison)\n",
    "        poisoned_X_train_image = attack_f['poisoned_X_train_image']\n",
    "        for counter, idx_to_poison in enumerate(indices_to_poison):\n",
    "            image_diff = np.max(np.abs(image_data_sets.train.x[idx_to_poison, :] - poisoned_X_train_image[counter, :]) * 255 / 2)        \n",
    "            assert image_diff < max_deviation + 1e-5\n",
    "        assert np.all(poisoned_X_train_image >= -1)\n",
    "        assert np.all(poisoned_X_train_image <= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test predictions flipped as the number of training images attacked increases:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_train_attacks_needed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a40e6f438b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of test predictions flipped as the number of training images attacked increases:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_train_attacks_needed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_orig_correct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'num_train_attacks_needed' is not defined"
     ]
    }
   ],
   "source": [
    "print('Number of test predictions flipped as the number of training images attacked increases:')\n",
    "pd.Series(num_train_attacks_needed[mask_orig_correct]).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
